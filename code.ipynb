{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper Demonstration Notebook using OpenAI's Whisper Implementation with GPU Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Check for GPU Availability and Load the Whisper Model on GPU if Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mengzheng Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model = whisper.load_model(\"turbo\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Define Function to Load and Transcribe Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(file_path):\n",
    "    \"\"\"\n",
    "    Load the audio with librosa, convert to 16kHz, and transcribe using Whisper.\n",
    "    \"\"\"\n",
    "    # Load audio file and resample to 16kHz\n",
    "    audio, sr = librosa.load(file_path, sr=16000)\n",
    "    \n",
    "    # Convert audio to the tensor format Whisper expects and move it to the GPU if available\n",
    "    audio_tensor = torch.tensor(audio).to(device)\n",
    "    \n",
    "    # Transcribe the audio tensor\n",
    "    result = model.transcribe(audio_tensor)\n",
    "    transcription = result['text']\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Define Function to Translate Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_audio(file_path, target_language=\"de\"):  # For example, 'de' for German\n",
    "    \"\"\"\n",
    "    Load the audio with librosa, convert to 16kHz, and translate using Whisper.\n",
    "    \"\"\"\n",
    "    # Load audio file and resample to 16kHz\n",
    "    audio, sr = librosa.load(file_path, sr=16000)\n",
    "    \n",
    "    # Convert audio to the tensor format Whisper expects and move it to the GPU if available\n",
    "    audio_tensor = torch.tensor(audio).to(device)\n",
    "    \n",
    "    # Translate the audio tensor by setting the task to 'translate'\n",
    "    result = model.transcribe(audio_tensor, task=\"translate\", language=target_language)\n",
    "    translation = result['text']\n",
    "    return translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First demo: Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Recording.wav\"\n",
    "# Transcription\n",
    "try:\n",
    "    transcription = transcribe_audio(file_path)\n",
    "    print(\"Transcription:\", transcription)\n",
    "except Exception as e:\n",
    "    print(\"Error during transcription:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second demo: Transcribing and translating from Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Recording_zh.wav\"\n",
    "# Translation\n",
    "try:\n",
    "    target_language = \"en\"  # e.g., 'fr' for French, 'es' for Spanish\n",
    "    transcription = transcribe_audio(file_path)\n",
    "    print(\"Transcription:\", transcription)\n",
    "    translation = translate_audio(file_path, target_language)\n",
    "    print(f\"Translation ({target_language}):\", translation)\n",
    "except Exception as e:\n",
    "    print(\"Error during translation:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
